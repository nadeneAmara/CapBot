{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CaptBot.ipynb",
      "provenance": [],
      "mount_file_id": "1Z74PgggjHkH6Wft2Mj9K6p1tiBqN4K65",
      "authorship_tag": "ABX9TyMYOhpRcSQWWs5pECxxUSdy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nadeneAmara/CaptBot/blob/master/CaptBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrtqSzbHtBw5",
        "outputId": "8d187179-e257-496b-8435-08c4d83e32f5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEMHsPxGPfen",
        "outputId": "c02e8a45-d0f1-438e-fe11-a1dce37b300c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJfQki6Qtm61"
      },
      "source": [
        "from __future__ import division\n",
        "import string\n",
        "import nltk, re, pprint\n",
        "from nltk import tokenize\n",
        "from nltk import word_tokenize\n",
        "from urllib import request\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from pickle import dump\n",
        "import requests\n",
        "\n",
        "# Retrieve comments from pushshift reddit database\n",
        "def request_comments(**kwargs):\n",
        "  response = requests.get(\"https://api.pushshift.io/reddit/comment/search/\",params=kwargs)\n",
        "  data = response.json()\n",
        "  comments = data['data']\n",
        "  return comments\n",
        "\n",
        "# Get text from comments\n",
        "def get_comment_set(comment_number):\n",
        "  sr_names = [\"aww\",\"wholesomememes\",\"funny\", \"interestingasfuck\", \"EarthPorn\"]\n",
        "  comment_bodies = \"\"\n",
        "  before = None\n",
        "  for i in sr_names:\n",
        "    comments_left = comment_number\n",
        "    while (comments_left > 0):\n",
        "      comments = request_comments(subreddit=i, size=100, before=before, sort='desc',sort_type='created_utc')\n",
        "      for comment in comments:\n",
        "        comment_bodies = comment_bodies + comment['body']\n",
        "        before = comment['created_utc']\n",
        "      comments_left = comments_left - 100\n",
        "      time.sleep(2)\n",
        "  return comment_bodies\n",
        "\n",
        "# Save comments to file, line by line\n",
        "def save_doc(sequences, filename):\n",
        "\tdataset = '\\n'.join(sequences)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(dataset)\n",
        "\tfile.close()\n",
        "\n",
        "# Load comments from file\n",
        "def load_doc(filename):\n",
        "\tfile = open(filename, 'r')\n",
        "\ttext = file.read()\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "def load_text():\n",
        "  # Make text all lowercase and split into sentences\n",
        "  # Load in new raw here\n",
        "  raw = get_comment_set(200)\n",
        "  raw = raw.lower()\n",
        "  raw_len = len(raw)\n",
        "  raw = tokenize.sent_tokenize(raw)\n",
        "  print(len(raw))\n",
        "  return raw\n",
        "\n",
        "# Generate overlapping sequences of words\n",
        "def get_sequences(raw):\n",
        "    sequences = []\n",
        "    maxLen = 0\n",
        "    for sequence in raw:\n",
        "        token_list = word_tokenize(sequence)\n",
        "        token_list = [token for token in token_list if token.isalpha()]\n",
        "        i = 0\n",
        "        while (i < (len(token_list)-1)):\n",
        "            tokens = token_list[:i+1]\n",
        "            line = ' '.join(tokens)\n",
        "            sequences.append(line)\n",
        "            i = i + 1\n",
        "    filename = 'reddit_comments.txt'\n",
        "    save_doc(sequences, filename)\n",
        "    return sequences\n",
        "\n",
        "# Map our words to integer values and split sequences into \n",
        "def prepare_sequences(sequences):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(sequences)\n",
        "    sequences = tokenizer.texts_to_sequences(sequences)\n",
        "    maxLen = max([len(x) for x in sequences])\n",
        "    num_vocab = len(tokenizer.word_index) + 1\n",
        "    input_sequences = np.array(pad_sequences(sequences, maxlen = maxLen-1, padding = 'pre'))\n",
        "    x = input_sequences[:,:-1]\n",
        "    y = input_sequences[:,-1]\n",
        "    y = to_categorical(y, num_classes=num_vocab)\n",
        "    len_sequence = x.shape[1]\n",
        "    # save the tokenizer\n",
        "    dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
        "    return x, y, len_sequence, num_vocab\n",
        "\n",
        "def create_model(maxLen, num_features):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(num_features, 50, input_length=maxLen))\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(num_features, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "#text = load_text()\n",
        "#s = get_sequences(text)\n",
        "filename = 'reddit_comments.txt'\n",
        "sequences = load_doc(filename)\n",
        "sequences = sequences.split('\\n')\n",
        "x, y, maxLen, num_features = prepare_sequences(sequences)\n",
        "model = create_model(maxLen, num_features)\n",
        "\n",
        "path = F\"/content/drive/MyDrive/captbot.ckpt\" \n",
        "#Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=path,\n",
        "                                                 save_weights_only=False,\n",
        "                                                 verbose=1)\n",
        "model.fit(x, y, epochs=150, callbacks=[cp_callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veSdgaVQhj5F"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWWs0FvBg92Q"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import random\n",
        "\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "\tresult = list()\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)\n",
        " \n",
        "def generate_seq_diverse(model, tokenizer, seq_length, seed_text, n_words):\n",
        "  result = list()\n",
        "  in_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "  for _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "    probabilities = model.predict(encoded)\n",
        "    predictions = []\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      predictions.append({'text': in_text + ' ' + word, \n",
        "                          'score': probabilities[0][index]})\n",
        "    predictions = sorted(predictions, key=lambda p: p['score'], reverse=True)\n",
        "    top_predictions = []\n",
        "    top_score = predictions[0]['score']\n",
        "    min_score = 0.6\n",
        "    rand_value = random.randint(int(min_score * 1000),1000)\n",
        "    for p in predictions:\n",
        "      if p['score'] >= rand_value/1000*top_score:\n",
        "        top_predictions.append(p)\n",
        "    random.shuffle(top_predictions)\n",
        "    in_text = top_predictions[0]['text']\n",
        "  return in_text\n",
        "\n",
        "# load model \n",
        "new_model = tf.keras.models.load_model(\"/content/drive/My Drive/captbot.ckpt\")\n",
        "# load tokenizer\n",
        "tokenizer = pickle.load(open('/content/drive/My Drive/tokenizer.pkl', 'rb'))\n",
        "generated = generate_seq(new_model, tokenizer, 149, \"A dog\", 10)\n",
        "generated2 = generate_seq_diverse(new_model, tokenizer, 149, \"dog\", 10)\n",
        "print(generated)\n",
        "print(generated2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}