{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CaptBot.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1Z74PgggjHkH6Wft2Mj9K6p1tiBqN4K65",
      "authorship_tag": "ABX9TyO4bM5bhdzmRZUnvRLBkPDz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nadeneAmara/CaptBot/blob/master/CaptBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrtqSzbHtBw5",
        "outputId": "ec9d2c1d-ce28-4bd3-d333-3965393e6922"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEMHsPxGPfen",
        "outputId": "930c39c2-bc8b-480e-ae7c-1027d7dad2cb"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJfQki6Qtm61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93c29b10-2526-4645-d86e-b20b729d10f1"
      },
      "source": [
        "from __future__ import division\n",
        "import string\n",
        "import nltk, re, pprint\n",
        "from nltk import tokenize\n",
        "from nltk import word_tokenize\n",
        "from urllib import request\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from pickle import dump\n",
        "\n",
        "# Retrieve comments from pushshift reddit database\n",
        "def request_comments(**kwargs):\n",
        "  response = requests.get(\"https://api.pushshift.io/reddit/comment/search/\",params=kwargs)\n",
        "  data = response.json()\n",
        "  comments = data['data']\n",
        "  return comments\n",
        "\n",
        "# Get text from comments\n",
        "def get_comment_set(comment_number):\n",
        "  sr_names = [\"\"]\n",
        "  comments_left = comment_number\n",
        "  comment_bodies = \"\"\n",
        "  for i in sr_names:\n",
        "    while (comments_left > 0):\n",
        "      comments = request_comments(subreddit=i, size=100, before=before, sort='desc',sort_type='created_utc')\n",
        "      for comment in comments:\n",
        "        comment_bodies = comment_bodies + comment['body']\n",
        "        before = comment['created_utc']\n",
        "      comments_left = comments_left - 100\n",
        "      time.sleep(2)\n",
        "  return comment_bodies\n",
        "\n",
        "\n",
        "def load_text(url):\n",
        "    response = request.urlopen(url)\n",
        "    raw = response.read().decode('utf8')\n",
        "    # Remove unwanted text\n",
        "    start = raw.find(\"Almustafa\")\n",
        "    end = raw.rfind(\"*** END OF THIS PROJECT\")\n",
        "    raw = raw[start:end]\n",
        "    # Make text all lowercase and split into sentences\n",
        "    # Load in new raw here\n",
        "    raw = raw.lower()\n",
        "    raw_len = len(raw)\n",
        "    raw = tokenize.sent_tokenize(raw)\n",
        "    print(len(raw))\n",
        "    return raw\n",
        "\n",
        "# Generate overlapping sequences of words\n",
        "def get_sequences(raw):\n",
        "    sequences = []\n",
        "    maxLen = 0\n",
        "    for sequence in raw:\n",
        "        token_list = word_tokenize(sequence)\n",
        "        token_list = [token for token in token_list if token.isalpha()]\n",
        "        i = 0\n",
        "        while (i < (len(token_list)-1)):\n",
        "            tokens = token_list[:i+1]\n",
        "            line = ' '.join(tokens)\n",
        "            sequences.append(line)\n",
        "            i = i + 1\n",
        "    return sequences\n",
        "\n",
        "# Map our words to integer values and split sequences into \n",
        "def prepare_sequences(sequences):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(sequences)\n",
        "    sequences = tokenizer.texts_to_sequences(sequences)\n",
        "    maxLen = max([len(x) for x in sequences])\n",
        "    num_vocab = len(tokenizer.word_index) + 1\n",
        "    input_sequences = np.array(pad_sequences(sequences, maxlen = maxLen-1, padding = 'pre'))\n",
        "    x = input_sequences[:,:-1]\n",
        "    y = input_sequences[:,-1]\n",
        "    y = to_categorical(y, num_classes=num_vocab)\n",
        "    len_sequence = x.shape[1]\n",
        "    # save the tokenizer\n",
        "    dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
        "    return x, y, len_sequence, num_vocab\n",
        "\n",
        "def create_model(maxLen, num_features):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(num_features, 50, input_length=maxLen))\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(num_features, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "# Load in The Prophet from Project Gutenberg \n",
        "url = \"http://www.gutenberg.org/files/58585/58585-0.txt\"\n",
        "text = load_text(url)\n",
        "sequences = get_sequences(text)\n",
        "x, y, maxLen, num_features = prepare_sequences(sequences)\n",
        "model = create_model(maxLen, num_features)\n",
        "\n",
        "#path = F\"/content/gdrive/My Drive/'captbot.ckpt'\" \n",
        "# Create a callback that saves the model's weights\n",
        "#cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=path,\n",
        "                                                 #save_weights_only=False,\n",
        "                                                 #verbose=1)\n",
        "#model.fit(x, y, epochs=100, callbacks=[cp_callback])\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWWs0FvBg92Q",
        "outputId": "ccb0b453-7713-456a-c539-af3bebd6f653"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import random\n",
        "\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "\tresult = list()\n",
        "\tin_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "\tfor _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
        "\t\t# map predicted word index to word\n",
        "\t\tout_word = ''\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == yhat:\n",
        "\t\t\t\tout_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\t# append to input\n",
        "\t\tin_text += ' ' + out_word\n",
        "\t\tresult.append(out_word)\n",
        "\treturn ' '.join(result)\n",
        " \n",
        "def generate_seq_diverse(model, tokenizer, seq_length, seed_text, n_words):\n",
        "  result = list()\n",
        "  in_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "  for _ in range(n_words):\n",
        "\t\t# encode the text as integer\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\t# truncate sequences to a fixed length\n",
        "    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\t\t# predict probabilities for each word\n",
        "    probabilities = model.predict(encoded)\n",
        "    predictions = []\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      predictions.append({'text': in_text + ' ' + word, \n",
        "                          'score': probabilities[0][index]})\n",
        "    predictions = sorted(predictions, key=lambda p: p['score'], reverse=True)\n",
        "    top_predictions = []\n",
        "    top_score = predictions[0]['score']\n",
        "    min_score = 0.1\n",
        "    rand_value = random.randint(int(min_score * 1000),1000)\n",
        "    for p in predictions:\n",
        "      if p['score'] >= rand_value/1000*top_score:\n",
        "        top_predictions.append(p)\n",
        "    random.shuffle(top_predictions)\n",
        "    in_text = top_predictions[0]['text']\n",
        "  return in_text\n",
        "\n",
        "# load model \n",
        "new_model = tf.keras.models.load_model(\"/content/drive/My Drive/'captbot.ckpt'\")\n",
        "# load tokenizer\n",
        "tokenizer = pickle.load(open('tokenizer.pkl', 'rb'))\n",
        "generated = generate_seq(new_model, tokenizer, 149, \"How\", 10)\n",
        "generated2 = generate_seq_diverse(new_model, tokenizer, 149, \"How could dog\", 10)\n",
        "print(generated)\n",
        "print(generated2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "can one be indeed near unless he be the condemned\n",
            "How could dog the condemned is the burden bearer for the guiltless and\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}